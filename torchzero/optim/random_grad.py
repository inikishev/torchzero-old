from typing import Optional
from collections.abc import Callable
import torch
from torch.optim import Optimizer

from .utils import foreach_param, foreach_group_param

class RandomGrad(Optimizer):
    def __init__(self, params, lr = 1e-5, opt = None, step_opposite = True, sampler: Callable = torch.randn_like):
        """Approximates gradients via random petrubation.
        This generates a random petrubation to model parameters and reevaluate the loss,
        if it increases, set `grad` attribute to petrubation, otherwise `grad` to minus petrubation.
        Then you can perform a step with any gradient-based optimizer.

        This evaluates closure twice per step.

        Args:
            params: Parameters to approximate gradients for, usually `model.parameters()`.

            lr (float): The magnitute of random petrubation, at smaller values gradients will be more accurate, but extremely small values don't seem to work well. Values generated by `sampler` will be multiplied by this. Defaults to 1e-5. Supports parameter groups.

            opt (torch.optim.Optimizer, optional): Optionally wrap an optimizer and perform a step with it after approximating gradients. Defaults to None.

            step_opposite (bool, optional): If `True`, if random petrubation increased loss, its negation will be used as gradient. If `False`, gradients will be zero. Defaults to True. Supports parameter groups.

            sampler (Callable, optional): Function that is used to generate random petrubation of size of a parameter, so functions like `torch.rand_like`. Defaults to torch.randn_like. Supports parameter groups.

        Example of approximating gradients and using Adam update rules:
        ```py
        optimizer = RandomGrad(model.parameters(), lr=1e-5, opt=optim.AdamW(MODEL.parameters(), lr=1e-3))
        @torch.no_grad # disable gradient calculations to speed it up
        def closure():
            optimizer.zero_grad()
            preds = model(inputs)
            loss = loss_fn(preds, targets)
            # loss.backward() - not needed
            return loss
        optimizer.step(closure)

        # alternatively you can define the approximator and the optimizer separately
        approximator = RandomGrad(model.parameters(), lr=1e-5, opt=None)
        optimizer = optim.AdamW(MODEL.parameters(), lr=1e-3)
        def closure():
            approximator.zero_grad() # or optimizer.zero_grad()
            preds = model(inputs)
            loss = loss_fn(preds, targets)
            # loss.backward() - not needed
            return loss
        approximator.step(closure)
        optimizer.step()
        """
        defaults = dict(
            lr = lr,
            sampler = sampler,
            step_opposite = step_opposite,
        )
        self.opt = opt
        super().__init__(params, defaults)

        self.lowest_loss = float("inf")
        self.n_steps = 0

    @torch.no_grad
    def step(self, closure:Callable): # type:ignore #pylint:disable=W0222
        """Performs a single optimization step (parameter update).

        Args:
            closure (Callable): A closure that reevaluates the model and returns the loss. The closure is evaluated twice per step.
        """
        # get initial loss on 1st step
        # on each step we calculate the loss, make a step, and calculate the loss again
        self.lowest_loss = closure()

        # make a step
        for group, p in foreach_group_param(self.param_groups):
            state = self.state[p]
            # save the params
            state['backup'] = p.clone()
            # create a random direction
            direction = group['sampler'](p, device=p.device) * group["lr"]
            state["direction"] = direction
            # add it
            p.add_(direction)

        # calculate a loss after stepping in the direction
        loss = closure()

        # if loss increased
        if loss > self.lowest_loss:
            # set gradients to anti-step
            for group, p in foreach_group_param(self.param_groups):
                state = self.state[p]

                # set the gradients
                if group["step_opposite"]:
                    if p.grad is None: p.grad = state["direction"] / group["lr"]
                    else: p.grad.add_(state["direction"] / group["lr"])

        # if loss decreased
        else:
            self.lowest_loss = loss
            # set gradients to step
            for group, p in foreach_group_param(self.param_groups):
                state = self.state[p]

                # set the gradients
                if p.grad is None: p.grad = -state["direction"] / group["lr"]
                else: p.grad.add_(-state["direction"] / group["lr"])

        # restore the params
        for group, p in foreach_group_param(self.param_groups):
            state = self.state[p]
            p.copy_(state['backup'])

        self.n_steps += 1
        if self.opt is not None: self.opt.step()
        return loss

class RandomBestGrad(Optimizer):
    def __init__(
        self,
        params,
        lr = 1e-5,
        opt = None,
        bestof:int = 10,
        avgbest = False,
        negate_worst = True,
        sampler: Callable = torch.randn_like,
    ):
        """Approximates gradients via random petrubations. This can also be thought of as particle swarm-based algorithm, where particles are sampled from the best point on each step.
        This generates a `bestof` random petrubations to model parameters and reevaluate the loss with each of them.
        `grad` attribute is set to minus best petrubation, or, if no petrubation decreased the loss, it will be set to the worst petrubation.
        Then you can perform a step with any gradient-based optimizer.

        This evaluates closure `bestof + 1` times per step.

        Args:
            params: Parameters to approximate gradients for, usually `model.parameters()`.

            lr (float): The magnitute of random petrubation, at smaller values gradients will be more accurate, but extremely small values don't seem to work well. Values generated by `sampler` will be multiplied by this. Defaults to 1e-5. Supports parameter groups.

            opt (torch.optim.Optimizer, optional): Optionally wrap an optimizer and perform a step with it after approximating gradients. Defaults to None.

            bestof (int, optional): This amount of petrubations will be generated each time. Therefore on each step loss will be reevaluated this many times. Defaults to 10.

            avgbest (bool, optional): If `True`, will set gradient to average of all petrubations that decreased the loss, if `False`, sets gradient to petrubation that decreased loss the most. Defaults to False. Supports parameter groups.

            negate_worst (bool, optional): If `True`, when no petrubations managed to decrease the loss, `grad` will be set to the petrubation that increased loss the most. If `False`, gradients will be zero. Defaults to True. Supports parameter groups.

            sampler (Callable, optional): Function that is used to generate random petrubation of size of a parameter, so functions like `torch.rand_like`. Defaults to torch.randn_like. Supports parameter groups.

        """
        defaults = dict(
            lr=lr,
            sampler=sampler,
            avgbest=avgbest,
            negate_worst=negate_worst,
        )
        self.opt = opt
        super().__init__(params, defaults)
        self.bestof = bestof

        self.lowest_loss = float("inf")
        self.n_steps = 0

        self.max_loss_increase = 0
        self.max_loss_decrease = 0


    @torch.no_grad
    def step(self, closure:Callable): # type:ignore #pylint:disable=W0222
        """Performs a single optimization step (parameter update).

        Args:
            closure (Callable): A closure that reevaluates the model and returns the loss. The closure is evaluated `bestof + 1` times per step.
        """
        # get baseline loss
        self.lowest_loss = closure()

        # backup params
        for group, p in foreach_group_param(self.param_groups):
            state = self.state[p]
            state['backup'] = p.clone()
            state["nbest"] = 0
            state["best"] = torch.zeros_like(p)

        self.max_loss_increase = 0
        self.max_loss_decrease = 0

        # generate `bestof` directions
        for step in range(self.bestof):
            # make a step
            for group, p in foreach_group_param(self.param_groups):
                state = self.state[p]
                # create a random direction
                direction = group['sampler'](p, device=p.device) * group["lr"]
                state["direction"] = direction
                # add it
                p.add_(direction)

            # calculate a loss after stepping in the direction
            loss = closure()

            # if loss increased
            if loss > self.lowest_loss:
                # if loss increased by more than all previous steps
                if loss - self.lowest_loss > self.max_loss_increase:
                    self.max_loss_increase = loss - self.lowest_loss
                    for group, p in foreach_group_param(self.param_groups):
                        # save worst parameters if negate_worst is True
                        if group["negate_worst"]:
                            state = self.state[p]
                            state["worst"] = state["direction"].clone()

            # if loss decreased
            else:
                # if loss decreased by more than all previous steps
                if self.lowest_loss - loss > self.max_loss_decrease:
                    self.max_loss_decrease = self.lowest_loss - loss
                    best = True
                else: best = False
                # save best parameters, or add them for averaging if `avgbest` is True
                for group, p in foreach_group_param(self.param_groups):
                    state = self.state[p]
                    state["nbest"] += 1
                    # add if avgbest
                    if group["avgbest"]:
                        state["best"].add_(state["direction"])
                    # else set if this direction is best so far
                    elif best: state["best"] = state["direction"].clone()

            # restore the params before next step
            for group, p in foreach_group_param(self.param_groups):
                state = self.state[p]
                p.copy_(state['backup'])

        # average best, negate if no good
        for group, p in foreach_group_param(self.param_groups):
            state = self.state[p]
            # average good steps
            if group["avgbest"] and state["nbest"] > 0: state["best"].div_(state["nbest"])
            # negate worst if no good steps and negate_worst is True
            if group["negate_worst"] and state["nbest"] == 0: state["best"] = - state["worst"]
            # set the gradients
            if p.grad is None: p.grad = -state["best"] / group["lr"]
            else: p.grad.add_(-state["best"] / group["lr"])

        self.n_steps += 1
        if self.opt is not None: self.opt.step()
        return loss#type:ignore
